{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langgraph\n",
      "  Using cached langgraph-0.2.35-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting bs4\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting PyYAML>=5.3 (from langchain_community)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain_community)\n",
      "  Downloading SQLAlchemy-2.0.35-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
      "  Downloading aiohttp-3.10.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.10 (from langchain_community)\n",
      "  Using cached langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.125 (from langchain_community)\n",
      "  Using cached langsmith-0.1.134-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain_community)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Using cached pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting requests<3,>=2 (from langchain_community)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain_community)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting openai<2.0.0,>=1.40.0 (from langchain-openai)\n",
      "  Using cached openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.0 (from langgraph)\n",
      "  Using cached langgraph_checkpoint-2.0.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading yarl-1.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (52 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.10->langchain_community)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain_community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain_community) (4.12.2)\n",
      "Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph)\n",
      "  Downloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.125->langchain_community)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain_community)\n",
      "  Downloading orjson-3.10.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain_community)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
      "  Using cached anyio-4.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
      "  Downloading jiter-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain_community)\n",
      "  Downloading charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain_community)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain_community)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain_community)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain_community)\n",
      "  Downloading greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai)\n",
      "  Downloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community)\n",
      "  Using cached httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain_community)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading propcache-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Using cached langchain_community-0.3.2-py3-none-any.whl (2.4 MB)\n",
      "Using cached langchain_openai-0.2.2-py3-none-any.whl (49 kB)\n",
      "Using cached langchain-0.3.3-py3-none-any.whl (1.0 MB)\n",
      "Using cached langgraph-0.2.35-py3-none-any.whl (108 kB)\n",
      "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading aiohttp-3.10.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached langchain_core-0.3.10-py3-none-any.whl (404 kB)\n",
      "Using cached langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Using cached langgraph_checkpoint-2.0.1-py3-none-any.whl (22 kB)\n",
      "Using cached langsmith-0.1.134-py3-none-any.whl (295 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Using cached openai-1.51.2-py3-none-any.whl (383 kB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading SQLAlchemy-2.0.35-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.6.0-py3-none-any.whl (89 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
      "Downloading greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (602 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m602.4/602.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading jiter-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Downloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Downloading orjson-3.10.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.8/792.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading yarl-1.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading propcache-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: urllib3, tqdm, tenacity, soupsieve, sniffio, regex, PyYAML, python-dotenv, pydantic-core, propcache, orjson, numpy, mypy-extensions, multidict, msgpack, marshmallow, jsonpointer, jiter, idna, h11, greenlet, frozenlist, distro, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, pydantic, jsonpatch, httpcore, beautifulsoup4, anyio, aiosignal, tiktoken, requests-toolbelt, pydantic-settings, httpx, dataclasses-json, bs4, aiohttp, openai, langsmith, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langgraph, langchain, langchain_community\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.0 attrs-24.2.0 beautifulsoup4-4.12.3 bs4-0.0.2 certifi-2024.8.30 charset-normalizer-3.4.0 dataclasses-json-0.6.7 distro-1.9.0 frozenlist-1.4.1 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 idna-3.10 jiter-0.6.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.3 langchain-core-0.3.10 langchain-openai-0.2.2 langchain-text-splitters-0.3.0 langchain_community-0.3.2 langgraph-0.2.35 langgraph-checkpoint-2.0.1 langsmith-0.1.134 marshmallow-3.22.0 msgpack-1.1.0 multidict-6.1.0 mypy-extensions-1.0.0 numpy-1.26.4 openai-1.51.2 orjson-3.10.7 propcache-0.2.0 pydantic-2.9.2 pydantic-core-2.23.4 pydantic-settings-2.5.2 python-dotenv-1.0.1 regex-2024.9.11 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 soupsieve-2.6 tenacity-8.5.0 tiktoken-0.8.0 tqdm-4.66.5 typing-inspect-0.9.0 urllib3-2.2.3 yarl-1.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain_community langchain-openai langchain langgraph bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "_set_env(\"AIML_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crawl4ai in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (0.3.5)\n",
      "Requirement already satisfied: aiosqlite==0.20.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from crawl4ai) (0.20.0)\n",
      "Requirement already satisfied: html2text==2024.2.26 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from crawl4ai) (2024.2.26)\n",
      "Requirement already satisfied: lxml==5.3.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from crawl4ai) (5.3.0)\n",
      "Requirement already satisfied: litellm==1.48.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from crawl4ai) (1.48.0)\n",
      "Requirement already satisfied: numpy<2.1.1,>=1.26.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from crawl4ai) (1.26.4)\n",
      "Requirement already satisfied: pillow==10.4.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from crawl4ai) (10.4.0)\n",
      "Requirement already satisfied: playwright==1.47.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from crawl4ai) (1.47.0)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from crawl4ai) (1.0.1)\n",
      "Requirement already satisfied: requests<2.32.3,>=2.26.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from crawl4ai) (2.32.2)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.3 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from crawl4ai) (4.12.3)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from aiosqlite==0.20.0->crawl4ai) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from beautifulsoup4==4.12.3->crawl4ai) (2.6)\n",
      "Requirement already satisfied: aiohttp in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from litellm==1.48.0->crawl4ai) (3.10.10)\n",
      "Requirement already satisfied: click in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from litellm==1.48.0->crawl4ai) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from litellm==1.48.0->crawl4ai) (8.5.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from litellm==1.48.0->crawl4ai) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from litellm==1.48.0->crawl4ai) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.45.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from litellm==1.48.0->crawl4ai) (1.51.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from litellm==1.48.0->crawl4ai) (2.9.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from litellm==1.48.0->crawl4ai) (0.8.0)\n",
      "Requirement already satisfied: tokenizers in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from litellm==1.48.0->crawl4ai) (0.20.1)\n",
      "Requirement already satisfied: greenlet==3.0.3 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from playwright==1.47.0->crawl4ai) (3.0.3)\n",
      "Requirement already satisfied: pyee==12.0.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from playwright==1.47.0->crawl4ai) (12.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from requests<2.32.3,>=2.26.0->crawl4ai) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from requests<2.32.3,>=2.26.0->crawl4ai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from requests<2.32.3,>=2.26.0->crawl4ai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from requests<2.32.3,>=2.26.0->crawl4ai) (2024.8.30)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm==1.48.0->crawl4ai) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.48.0->crawl4ai) (3.0.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.48.0->crawl4ai) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.48.0->crawl4ai) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.48.0->crawl4ai) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.48.0->crawl4ai) (0.20.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from openai>=1.45.0->litellm==1.48.0->crawl4ai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from openai>=1.45.0->litellm==1.48.0->crawl4ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from openai>=1.45.0->litellm==1.48.0->crawl4ai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from openai>=1.45.0->litellm==1.48.0->crawl4ai) (0.6.1)\n",
      "Requirement already satisfied: sniffio in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from openai>=1.45.0->litellm==1.48.0->crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from openai>=1.45.0->litellm==1.48.0->crawl4ai) (4.66.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm==1.48.0->crawl4ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm==1.48.0->crawl4ai) (2.23.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm==1.48.0->crawl4ai) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from aiohttp->litellm==1.48.0->crawl4ai) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from aiohttp->litellm==1.48.0->crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from aiohttp->litellm==1.48.0->crawl4ai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from aiohttp->litellm==1.48.0->crawl4ai) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from aiohttp->litellm==1.48.0->crawl4ai) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from tokenizers->litellm==1.48.0->crawl4ai) (0.25.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.45.0->litellm==1.48.0->crawl4ai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.45.0->litellm==1.48.0->crawl4ai) (0.14.0)\n",
      "Requirement already satisfied: filelock in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.48.0->crawl4ai) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.48.0->crawl4ai) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.48.0->crawl4ai) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.48.0->crawl4ai) (6.0.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp->litellm==1.48.0->crawl4ai) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nest-asyncio in /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install crawl4ai\n",
    "%pip install nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Synchronous WebCrawler is not available. Install crawl4ai[sync] for synchronous support. However, please note that the synchronous version will be deprecated soon.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy\n",
    "import json\n",
    "import time\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] 🌤️  Warming up the AsyncWebCrawler\n",
      "[LOG] 🌞 AsyncWebCrawler is ready to crawl\n",
      "[LOG] 🕸️ Crawling https://langchain-ai.github.io/langgraph/how-tos/branching/ using AsyncPlaywrightCrawlerStrategy...\n",
      "[LOG] ✅ Crawled https://langchain-ai.github.io/langgraph/how-tos/branching/ successfully!\n",
      "[LOG] 🚀 Crawling done for https://langchain-ai.github.io/langgraph/how-tos/branching/, success: True, time taken: 0.91 seconds\n",
      "[LOG] 🚀 Content extracted for https://langchain-ai.github.io/langgraph/how-tos/branching/, success: True, time taken: 0.09 seconds\n",
      "[LOG] 🔥 Extracting semantic blocks for https://langchain-ai.github.io/langgraph/how-tos/branching/, Strategy: AsyncWebCrawler\n",
      "[LOG] 🚀 Extraction done for https://langchain-ai.github.io/langgraph/how-tos/branching/, time taken: 0.09 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Skip to content\\n\\n![\\\\\\\\\"logo\\\\\\\\\"](\\\\\\\\\"https://langchain-\\nai.github.io/langgraph/static/wordmark_dark.svg\\\\\\\\\"/)\\n![\\\\\\\\\"logo\\\\\\\\\"](\\\\\\\\\"https://langchain-\\nai.github.io/langgraph/static/wordmark_light.svg\\\\\\\\\"/)\\n\\nCreate branches for parallel execution\\n\\nInitializing search\\n\\nGitHub\\n\\n  * Home \\n  * Tutorials \\n  * How-to Guides \\n  * Conceptual Guides \\n  * Reference \\n  * Cloud (beta) \\n\\n![\\\\\\\\\"logo\\\\\\\\\"](\\\\\\\\\"https://langchain-\\nai.github.io/langgraph/static/wordmark_dark.svg\\\\\\\\\"/)\\n![\\\\\\\\\"logo\\\\\\\\\"](\\\\\\\\\"https://langchain-\\nai.github.io/langgraph/static/wordmark_light.svg\\\\\\\\\"/)\\n\\nGitHub\\n\\n  * Home \\n  * Tutorials \\n  * How-to Guides \\n\\nHow-to Guides\\n\\n    * Controllability  Controllability \\n      * Create branches for parallel execution  Create branches for parallel execution  Table of contents \\n        * Setup \\n        * Parallel node fan-out and fan-in \\n        * Parallel node fan-out and fan-in with extra steps \\n        * Conditional Branching \\n        * Stable Sorting \\n      * Create map-reduce branches for parallel execution \\n      * Control graph recursion limit \\n    * Persistence \\n    * Memory \\n    * Human-in-the-loop \\n    * Streaming \\n    * Tool calling \\n    * Subgraphs \\n    * State Management \\n    * Other \\n    * Prebuilt ReAct Agent \\n  * Conceptual Guides \\n  * Reference \\n  * Cloud (beta) \\n\\nTable of contents\\n\\n  * Setup \\n  * Parallel node fan-out and fan-in \\n  * Parallel node fan-out and fan-in with extra steps \\n  * Conditional Branching \\n  * Stable Sorting \\n\\n# How to create branches for parallel node execution¶\\n\\nPrerequisites\\n\\nThis guide assumes familiarity with the following:\\n\\n  * Node \\n  * Edge \\n  * Reducer \\n\\nParallel execution of nodes is essential to speed up overall graph operation.\\nLangGraph offers native support for parallel execution of nodes, which can\\nsignificantly enhance the performance of graph-based workflows. This\\nparallelization is achieved through fan-out and fan-in mechanisms, utilizing\\nboth standard edges and conditional_edges. Below are some examples showing how\\nto add create branching dataflows that work for you.\\n\\n![\\\\\\\\\"Screenshot](\\\\\\\\\"\\\\\\\\\"/)\\n\\n## Setup¶\\n\\nFirst, let\\\\\\'s install the required packages\\n\\n```\\n\\n    \\n    \\n    %%capture --no-stderr\\n    %pip install -U langgraph\\n    \\n```\\n\\nSet up LangSmith for LangGraph development\\n\\nSign up for LangSmith to quickly spot issues and improve the performance of\\nyour LangGraph projects. LangSmith lets you use trace data to debug, test, and\\nmonitor your LLM apps built with LangGraph — read more about how to get\\nstarted here.\\n\\n## Parallel node fan-out and fan-in¶\\n\\nIn this example, we fan out from Node A to B and C and then fan in to D. With\\nour state, we specify the reducer add operation. This will combine or\\naccumulate values for the specific key in the State, rather than simply\\noverwriting the existing value. For lists, this means concatenating the new\\nlist with the existing list.\\n\\nNote that LangGraph uses Annotated type to specify reducer functions for\\nspecific keys in the State: it maintains the original type (list) for type\\nchecking, but allows attaching the reducer function (add) to the type without\\nchanging the type itself.\\n\\n```\\n\\n    \\n    \\n    import operator\\n    from typing import Annotated, Any\\n    \\n    from typing_extensions import TypedDict\\n    \\n    from langgraph.graph import StateGraph, START, END\\n    \\n    \\n    class State(TypedDict):\\n      # The operator.add reducer fn makes this append-only\\n      aggregate: Annotated[list, operator.add]\\n    \\n    \\n    class ReturnNodeValue:\\n      def __init__(self, node_secret: str):\\n        self._value = node_secret\\n    \\n      def __call__(self, state: State) -> Any:\\n        print(f\\\\\"Adding {self._value} to {state[\\\\\\'aggregate\\\\\\']}\\\\\")\\n        return {\\\\\"aggregate\\\\\": [self._value]}\\n    \\n    \\n    builder = StateGraph(State)\\n    builder.add_node(\\\\\"a\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m A\\\\\"))\\n    builder.add_edge(START, \\\\\"a\\\\\")\\n    builder.add_node(\\\\\"b\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m B\\\\\"))\\n    builder.add_node(\\\\\"c\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m C\\\\\"))\\n    builder.add_node(\\\\\"d\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m D\\\\\"))\\n    builder.add_edge(\\\\\"a\\\\\", \\\\\"b\\\\\")\\n    builder.add_edge(\\\\\"a\\\\\", \\\\\"c\\\\\")\\n    builder.add_edge(\\\\\"b\\\\\", \\\\\"d\\\\\")\\n    builder.add_edge(\\\\\"c\\\\\", \\\\\"d\\\\\")\\n    builder.add_edge(\\\\\"d\\\\\", END)\\n    graph = builder.compile()\\n    \\n```\\n\\nAPI Reference: StateGraph | START | END\\n\\n```\\n\\n    \\n    \\n    from IPython.display import Image, display\\n    \\n    display(Image(graph.get_graph().draw_mermaid_png()))\\n    \\n```\\n\\n![\\\\\\\\\"\\\\\\\\\"](\\\\\\\\\"\\\\\\\\\"/)\\n\\nWith the reducer, you can see that the values added in each node are\\naccumulated.\\n\\n```\\n\\n    \\n    \\n    graph.invoke({\\\\\"aggregate\\\\\": []}, {\\\\\"configurable\\\\\": {\\\\\"thread_id\\\\\": \\\\\"foo\\\\\"}})\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    Adding I\\\\\\'m A to []\\n    Adding I\\\\\\'m B to [\\\\\"I\\\\\\'m A\\\\\"]\\n    Adding I\\\\\\'m C to [\\\\\"I\\\\\\'m A\\\\\"]\\n    Adding I\\\\\\'m D to [\\\\\"I\\\\\\'m A\\\\\", \\\\\"I\\\\\\'m B\\\\\", \\\\\"I\\\\\\'m C\\\\\"]\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    {\\\\\\'aggregate\\\\\\': [\\\\\"I\\\\\\'m A\\\\\", \\\\\"I\\\\\\'m B\\\\\", \\\\\"I\\\\\\'m C\\\\\", \\\\\"I\\\\\\'m D\\\\\"]}\\n    \\n```\\n\\nException handling?\\n\\nLangGraph executes nodes within \\\\\"supersteps\\\\\", meaning that while parallel\\nbranches are executed in parallel, the entire superstep is transactional. If\\nany of these branches raises an exception, none of the updates are applied to\\nthe state (the entire superstep errors). If you have error-prone (perhaps want\\nto handle flakey API calls), LangGraph provides two ways to address this:\\n\\n  1. You can write regular python code within your node to catch and handle exceptions.\\n  2. You can set a retry_policy to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn\\\\\\'t worry about performing redundant work.\\n\\nTogether, these let you perform parallel execution and fully control exception\\nhandling.\\n\\n## Parallel node fan-out and fan-in with extra steps¶\\n\\nThe above example showed how to fan-out and fan-in when each path was only one\\nstep. But what if one path had more than one step?\\n\\n```\\n\\n    \\n    \\n    import operator\\n    from typing import Annotated\\n    \\n    from typing_extensions import TypedDict\\n    \\n    from langgraph.graph import StateGraph\\n    \\n    \\n    class State(TypedDict):\\n      # The operator.add reducer fn makes this append-only\\n      aggregate: Annotated[list, operator.add]\\n    \\n    \\n    builder = StateGraph(State)\\n    builder.add_node(\\\\\"a\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m A\\\\\"))\\n    builder.add_edge(START, \\\\\"a\\\\\")\\n    builder.add_node(\\\\\"b\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m B\\\\\"))\\n    builder.add_node(\\\\\"b2\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m B2\\\\\"))\\n    builder.add_node(\\\\\"c\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m C\\\\\"))\\n    builder.add_node(\\\\\"d\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m D\\\\\"))\\n    builder.add_edge(\\\\\"a\\\\\", \\\\\"b\\\\\")\\n    builder.add_edge(\\\\\"a\\\\\", \\\\\"c\\\\\")\\n    builder.add_edge(\\\\\"b\\\\\", \\\\\"b2\\\\\")\\n    builder.add_edge([\\\\\"b2\\\\\", \\\\\"c\\\\\"], \\\\\"d\\\\\")\\n    builder.add_edge(\\\\\"d\\\\\", END)\\n    graph = builder.compile()\\n    \\n```\\n\\nAPI Reference: StateGraph\\n\\n```\\n\\n    \\n    \\n    from IPython.display import Image, display\\n    \\n    display(Image(graph.get_graph().draw_mermaid_png()))\\n    \\n```\\n\\n![\\\\\\\\\"\\\\\\\\\"](\\\\\\\\\"\\\\\\\\\"/)\\n\\n```\\n\\n    \\n    \\n    graph.invoke({\\\\\"aggregate\\\\\": []})\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    Adding I\\\\\\'m A to []\\n    Adding I\\\\\\'m B to [\\\\\"I\\\\\\'m A\\\\\"]\\n    Adding I\\\\\\'m C to [\\\\\"I\\\\\\'m A\\\\\"]\\n    Adding I\\\\\\'m B2 to [\\\\\"I\\\\\\'m A\\\\\", \\\\\"I\\\\\\'m B\\\\\", \\\\\"I\\\\\\'m C\\\\\"]\\n    Adding I\\\\\\'m D to [\\\\\"I\\\\\\'m A\\\\\", \\\\\"I\\\\\\'m B\\\\\", \\\\\"I\\\\\\'m C\\\\\", \\\\\"I\\\\\\'m B2\\\\\"]\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    {\\\\\\'aggregate\\\\\\': [\\\\\"I\\\\\\'m A\\\\\", \\\\\"I\\\\\\'m B\\\\\", \\\\\"I\\\\\\'m C\\\\\", \\\\\"I\\\\\\'m B2\\\\\", \\\\\"I\\\\\\'m D\\\\\"]}\\n    \\n```\\n\\n## Conditional Branching¶\\n\\nIf your fan-out is not deterministic, you can use add_conditional_edges\\ndirectly.\\n\\nIf you have a known \\\\\"sink\\\\\" node that the conditional branches will route to\\nafterwards, you can provide then=<final-node-name> when creating the\\nconditional edges.\\n\\n```\\n\\n    \\n    \\n    import operator\\n    from typing import Annotated, Sequence\\n    \\n    from typing_extensions import TypedDict\\n    \\n    from langgraph.graph import END, START, StateGraph\\n    \\n    \\n    class State(TypedDict):\\n      # The operator.add reducer fn makes this append-only\\n      aggregate: Annotated[list, operator.add]\\n      which: str\\n    \\n    \\n    builder = StateGraph(State)\\n    builder.add_node(\\\\\"a\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m A\\\\\"))\\n    builder.add_edge(START, \\\\\"a\\\\\")\\n    builder.add_node(\\\\\"b\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m B\\\\\"))\\n    builder.add_node(\\\\\"c\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m C\\\\\"))\\n    builder.add_node(\\\\\"d\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m D\\\\\"))\\n    builder.add_node(\\\\\"e\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m E\\\\\"))\\n    \\n    \\n    def route_bc_or_cd(state: State) -> Sequence[str]:\\n      if state[\\\\\"which\\\\\"] == \\\\\"cd\\\\\":\\n        return [\\\\\"c\\\\\", \\\\\"d\\\\\"]\\n      return [\\\\\"b\\\\\", \\\\\"c\\\\\"]\\n    \\n    \\n    intermediates = [\\\\\"b\\\\\", \\\\\"c\\\\\", \\\\\"d\\\\\"]\\n    builder.add_conditional_edges(\\n      \\\\\"a\\\\\",\\n      route_bc_or_cd,\\n      intermediates,\\n    )\\n    for node in intermediates:\\n      builder.add_edge(node, \\\\\"e\\\\\")\\n    \\n    \\n    builder.add_edge(\\\\\"e\\\\\", END)\\n    graph = builder.compile()\\n    \\n```\\n\\nAPI Reference: END | START | StateGraph\\n\\n```\\n\\n    \\n    \\n    from IPython.display import Image, display\\n    \\n    display(Image(graph.get_graph().draw_mermaid_png()))\\n    \\n```\\n\\n![\\\\\\\\\"\\\\\\\\\"](\\\\\\\\\"\\\\\\\\\"/)\\n\\n```\\n\\n    \\n    \\n    graph.invoke({\\\\\"aggregate\\\\\": [], \\\\\"which\\\\\": \\\\\"bc\\\\\"})\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    Adding I\\\\\\'m A to []\\n    Adding I\\\\\\'m B to [\\\\\"I\\\\\\'m A\\\\\"]\\n    Adding I\\\\\\'m C to [\\\\\"I\\\\\\'m A\\\\\"]\\n    Adding I\\\\\\'m E to [\\\\\"I\\\\\\'m A\\\\\", \\\\\"I\\\\\\'m B\\\\\", \\\\\"I\\\\\\'m C\\\\\"]\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    {\\\\\\'aggregate\\\\\\': [\\\\\"I\\\\\\'m A\\\\\", \\\\\"I\\\\\\'m B\\\\\", \\\\\"I\\\\\\'m C\\\\\", \\\\\"I\\\\\\'m E\\\\\"], \\\\\\'which\\\\\\': \\\\\\'bc\\\\\\'}\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    graph.invoke({\\\\\"aggregate\\\\\": [], \\\\\"which\\\\\": \\\\\"cd\\\\\"})\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    Adding I\\\\\\'m A to []\\n    Adding I\\\\\\'m C to [\\\\\"I\\\\\\'m A\\\\\"]\\n    Adding I\\\\\\'m D to [\\\\\"I\\\\\\'m A\\\\\"]\\n    Adding I\\\\\\'m E to [\\\\\"I\\\\\\'m A\\\\\", \\\\\"I\\\\\\'m C\\\\\", \\\\\"I\\\\\\'m D\\\\\"]\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    {\\\\\\'aggregate\\\\\\': [\\\\\"I\\\\\\'m A\\\\\", \\\\\"I\\\\\\'m C\\\\\", \\\\\"I\\\\\\'m D\\\\\", \\\\\"I\\\\\\'m E\\\\\"], \\\\\\'which\\\\\\': \\\\\\'cd\\\\\\'}\\n    \\n```\\n\\n## Stable Sorting¶\\n\\nWhen fanned out, nodes are run in parallel as a single \\\\\"superstep\\\\\". The\\nupdates from each superstep are all applied to the state in sequence once the\\nsuperstep has completed.\\n\\nIf you need consistent, predetermined ordering of updates from a parallel\\nsuperstep, you should write the outputs (along with an identifying key) to a\\nseparate field in your state, then combine them in the \\\\\"sink\\\\\" node by adding\\nregular edge\\\\\\'s from each of the fanout nodes to the rendezvous point.\\n\\nFor instance, suppose I want to order the outputs of the parallel step by\\n\\\\\"reliability\\\\\".\\n\\n```\\n\\n    \\n    \\n    import operator\\n    from typing import Annotated, Sequence\\n    \\n    from typing_extensions import TypedDict\\n    \\n    from langgraph.graph import StateGraph\\n    \\n    \\n    def reduce_fanouts(left, right):\\n      if left is None:\\n        left = []\\n      if not right:\\n        # Overwrite\\n        return []\\n      return left + right\\n    \\n    \\n    class State(TypedDict):\\n      # The operator.add reducer fn makes this append-only\\n      aggregate: Annotated[list, operator.add]\\n      fanout_values: Annotated[list, reduce_fanouts]\\n      which: str\\n    \\n    \\n    builder = StateGraph(State)\\n    builder.add_node(\\\\\"a\\\\\", ReturnNodeValue(\\\\\"I\\\\\\'m A\\\\\"))\\n    builder.add_edge(START, \\\\\"a\\\\\")\\n    \\n    \\n    class ParallelReturnNodeValue:\\n      def __init__(\\n        self,\\n        node_secret: str,\\n        reliability: float,\\n      ):\\n        self._value = node_secret\\n        self._reliability = reliability\\n    \\n      def __call__(self, state: State) -> Any:\\n        print(f\\\\\"Adding {self._value} to {state[\\\\\\'aggregate\\\\\\']} in parallel.\\\\\")\\n        return {\\n          \\\\\"fanout_values\\\\\": [\\n            {\\n              \\\\\"value\\\\\": [self._value],\\n              \\\\\"reliability\\\\\": self._reliability,\\n            }\\n          ]\\n        }\\n    \\n    \\n    builder.add_node(\\\\\"b\\\\\", ParallelReturnNodeValue(\\\\\"I\\\\\\'m B\\\\\", reliability=0.9))\\n    \\n    builder.add_node(\\\\\"c\\\\\", ParallelReturnNodeValue(\\\\\"I\\\\\\'m C\\\\\", reliability=0.1))\\n    builder.add_node(\\\\\"d\\\\\", ParallelReturnNodeValue(\\\\\"I\\\\\\'m D\\\\\", reliability=0.3))\\n    \\n    \\n    def aggregate_fanout_values(state: State) -> Any:\\n      # Sort by reliability\\n      ranked_values = sorted(\\n        state[\\\\\"fanout_values\\\\\"], key=lambda x: x[\\\\\"reliability\\\\\"], reverse=True\\n      )\\n      return {\\n        \\\\\"aggregate\\\\\": [x[\\\\\"value\\\\\"] for x in ranked_values] + [\\\\\"I\\\\\\'m E\\\\\"],\\n        \\\\\"fanout_values\\\\\": [],\\n      }\\n    \\n    \\n    builder.add_node(\\\\\"e\\\\\", aggregate_fanout_values)\\n    \\n    \\n    def route_bc_or_cd(state: State) -> Sequence[str]:\\n      if state[\\\\\"which\\\\\"] == \\\\\"cd\\\\\":\\n        return [\\\\\"c\\\\\", \\\\\"d\\\\\"]\\n      return [\\\\\"b\\\\\", \\\\\"c\\\\\"]\\n    \\n    \\n    intermediates = [\\\\\"b\\\\\", \\\\\"c\\\\\", \\\\\"d\\\\\"]\\n    builder.add_conditional_edges(\\\\\"a\\\\\", route_bc_or_cd, intermediates)\\n    \\n    for node in intermediates:\\n      builder.add_edge(node, \\\\\"e\\\\\")\\n    \\n    builder.add_edge(\\\\\"e\\\\\", END)\\n    graph = builder.compile()\\n    \\n```\\n\\nAPI Reference: StateGraph\\n\\n```\\n\\n    \\n    \\n    from IPython.display import Image, display\\n    \\n    display(Image(graph.get_graph().draw_mermaid_png()))\\n    \\n```\\n\\n![\\\\\\\\\"\\\\\\\\\"](\\\\\\\\\"\\\\\\\\\"/)\\n\\n```\\n\\n    \\n    \\n    graph.invoke({\\\\\"aggregate\\\\\": [], \\\\\"which\\\\\": \\\\\"bc\\\\\", \\\\\"fanout_values\\\\\": []})\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    Adding I\\\\\\'m A to []\\n    Adding I\\\\\\'m B to [\\\\\"I\\\\\\'m A\\\\\"] in parallel.\\n    Adding I\\\\\\'m C to [\\\\\"I\\\\\\'m A\\\\\"] in parallel.\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    {\\\\\\'aggregate\\\\\\': [\\\\\"I\\\\\\'m A\\\\\", [\\\\\"I\\\\\\'m B\\\\\"], [\\\\\"I\\\\\\'m C\\\\\"], \\\\\"I\\\\\\'m E\\\\\"],\\n     \\\\\\'fanout_values\\\\\\': [],\\n     \\\\\\'which\\\\\\': \\\\\\'bc\\\\\\'}\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    graph.invoke({\\\\\"aggregate\\\\\": [], \\\\\"which\\\\\": \\\\\"cd\\\\\"})\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    Adding I\\\\\\'m A to []\\n    Adding I\\\\\\'m C to [\\\\\"I\\\\\\'m A\\\\\"] in parallel.\\n    Adding I\\\\\\'m D to [\\\\\"I\\\\\\'m A\\\\\"] in parallel.\\n    \\n```\\n\\n```\\n\\n    \\n    \\n    {\\\\\\'aggregate\\\\\\': [\\\\\"I\\\\\\'m A\\\\\", [\\\\\"I\\\\\\'m D\\\\\"], [\\\\\"I\\\\\\'m C\\\\\"], \\\\\"I\\\\\\'m E\\\\\"],\\n     \\\\\\'fanout_values\\\\\\': [],\\n     \\\\\\'which\\\\\\': \\\\\\'cd\\\\\\'}\\n    \\n```\\n\\n## Comments\\n\\nBack to top\\n\\nPrevious\\n\\nHow-to guides\\n\\nNext\\n\\nCreate map-reduce branches for parallel execution\\n\\nMade with  Material for MkDocs\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def simple_crawl(url):\n",
    "    async with AsyncWebCrawler(verbose=True) as crawler:\n",
    "        result = await crawler.arun(url=url)\n",
    "        return result.markdown\n",
    "result = asyncio.run(simple_crawl(\"https://langchain-ai.github.io/langgraph/how-tos/branching/\"))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# Langgraph how tos docs\n",
    "url = \"https://langchain-ai.github.io/langgraph/how-tos/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_how_to = loader.load()\n",
    "\n",
    "# Langgraph tutorial docs\n",
    "url = \"https://langchain-ai.github.io/langgraph/tutorials/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_tutorial = loader.load()\n",
    "\n",
    "# # Sort the list based on the URLs and get the text\n",
    "# d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "# d_reversed = list(reversed(d_sorted))\n",
    "# concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "#     [doc.page_content for doc in d_reversed]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix='To build a parallel branch in LangGraph, you can use the fan-out and fan-in mechanism. This involves creating nodes that branch out from a single node and then converge back into another node. The process is facilitated by the StateGraph class, which allows you to define nodes and edges, and specify how data should be aggregated using reducers. In this example, we create a graph where Node A fans out to Nodes B and C, and then both converge into Node D. The reducer function is used to accumulate values from each node into a list.', imports='import operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END', code='class State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\nclass ReturnNodeValue:\\n    def __init__(self, node_secret: str):\\n        self._value = node_secret\\n\\n    def __call__(self, state: State) -> Any:\\n        print(f\"Adding {self._value} to {state[\\'aggregate\\']}\")\\n        return {\"aggregate\": [self._value]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"a\", ReturnNodeValue(\"I\\'m A\"))\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_node(\"b\", ReturnNodeValue(\"I\\'m B\"))\\nbuilder.add_node(\"c\", ReturnNodeValue(\"I\\'m C\"))\\nbuilder.add_node(\"d\", ReturnNodeValue(\"I\\'m D\"))\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\n# To visualize the graph\\nfrom IPython.display import Image, display\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n# Invoke the graph\\nresult = graph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\\nprint(result)')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "# Grader prompt\n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a coding assistant with expertise in LangGraph. \\n \n",
    "    Here is a full set of LangGraph how tos and tutorials documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user question based on the above provided documentation. Ensure any code you provide can be executed \\n \n",
    "    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
    "    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "\n",
    "\n",
    "expt_llm = \"gpt-4o\"\n",
    "llm = ChatOpenAI(temperature=0, model=expt_llm)\n",
    "code_gen_chain_oai = code_gen_prompt | llm.with_structured_output(code)\n",
    "question = \"How do I build a parallel branch in LangGraph?\"\n",
    "solution = code_gen_chain_oai.invoke(\n",
    "    {\"context\": result, \"messages\": [(\"user\", question)]}\n",
    ")\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(temperature=1, model=\"o1-mini\", base_url=\"https://api.aimlapi.com/v1\", api_key=os.environ[\"AIML_API_KEY\"])\n",
    "\n",
    "# Create a simple prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create a simple chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Test the chain\n",
    "response = chain.invoke({\"input\": \"What are the 5 most common questions users ask about LangGraph? If you don't know the answer, just say so.\"})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have specific information about LangGraph. It's possible that it's a tool or platform that emerged after my last update in October 2023, or it might be a specialized tool not widely documented. If you can provide more details or context about LangGraph, I'd be happy to try to help further!\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix='To build a RAG (Retrieval-Augmented Generation) chain in LangGraph, you need to create a graph that retrieves information from a knowledge base and then uses that information to generate a response. This involves setting up nodes for retrieval and generation, and connecting them with edges to form a chain. The retrieval node will query the knowledge base, and the generation node will use the retrieved information to produce a response. Below is an example of how you can set up such a chain in LangGraph.', imports='from langgraph.graph import StateGraph, START, END\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nimport operator', code='class State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n    retrieved_info: str\\n\\nclass RetrieveNode:\\n    def __init__(self, query: str):\\n        self.query = query\\n\\n    def __call__(self, state: State) -> Any:\\n        # Simulate retrieval from a knowledge base\\n        retrieved_data = f\"Information related to {self.query}\"\\n        print(f\"Retrieved: {retrieved_data}\")\\n        return {\"retrieved_info\": retrieved_data}\\n\\nclass GenerateNode:\\n    def __call__(self, state: State) -> Any:\\n        # Use the retrieved information to generate a response\\n        response = f\"Generated response using {state[\\'retrieved_info\\']}\"\\n        print(f\"Generated: {response}\")\\n        return {\"aggregate\": [response]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"retrieve\", RetrieveNode(\"example query\"))\\nbuilder.add_edge(START, \"retrieve\")\\nbuilder.add_node(\"generate\", GenerateNode())\\nbuilder.add_edge(\"retrieve\", \"generate\")\\nbuilder.add_edge(\"generate\", END)\\ngraph = builder.compile()\\n\\n# Execute the graph\\nresult = graph.invoke({\"aggregate\": []})\\nprint(result)')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "question = \"How do I build a RAG chain in LangGraph?\"\n",
    "solution = code_gen_chain_oai.invoke(\n",
    "    {\"context\": result, \"messages\": [(\"user\", question)]}\n",
    ")\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix='To build a RAG (Retrieval-Augmented Generation) chain in LangGraph, you need to create a graph that retrieves information from a knowledge base and then uses that information to generate a response. This involves setting up nodes for retrieval and generation, and connecting them with edges to form a chain. The retrieval node will query the knowledge base, and the generation node will use the retrieved information to produce a response. Below is an example of how you can set up such a chain in LangGraph.' imports='from langgraph.graph import StateGraph, START, END\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nimport operator' code='class State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n    retrieved_info: str\\n\\nclass RetrieveNode:\\n    def __init__(self, query: str):\\n        self.query = query\\n\\n    def __call__(self, state: State) -> Any:\\n        # Simulate retrieval from a knowledge base\\n        retrieved_data = f\"Information related to {self.query}\"\\n        print(f\"Retrieved: {retrieved_data}\")\\n        return {\"retrieved_info\": retrieved_data}\\n\\nclass GenerateNode:\\n    def __call__(self, state: State) -> Any:\\n        # Use the retrieved information to generate a response\\n        response = f\"Generated response using {state[\\'retrieved_info\\']}\"\\n        print(f\"Generated: {response}\")\\n        return {\"aggregate\": [response]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"retrieve\", RetrieveNode(\"example query\"))\\nbuilder.add_edge(START, \"retrieve\")\\nbuilder.add_node(\"generate\", GenerateNode())\\nbuilder.add_edge(\"retrieve\", \"generate\")\\nbuilder.add_edge(\"generate\", END)\\ngraph = builder.compile()\\n\\n# Execute the graph\\nresult = graph.invoke({\"aggregate\": []})\\nprint(result)'\n"
     ]
    }
   ],
   "source": [
    "# o1 Reasoning\n",
    "# take 5 random docs and ask complex questions that a user might ask\n",
    "# generate code for the question using these docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        error : Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages : With user question, error messages, reasoning\n",
    "        generation : Code solution\n",
    "        iterations : Number of tries\n",
    "    \"\"\"\n",
    "\n",
    "    error: str\n",
    "    messages: List\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_gen_chain = code_gen_chain_oai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameter\n",
    "\n",
    "# Max tries\n",
    "max_iterations = 3\n",
    "# Reflect\n",
    "# flag = 'reflect'\n",
    "flag = \"do not reflect\"\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate a code solution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    error = state[\"error\"]\n",
    "\n",
    "    # We have been routed back to generation with an error\n",
    "    if error == \"yes\":\n",
    "        messages += [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Solution\n",
    "    code_solution = code_gen_chain.invoke(\n",
    "        {\"context\": result, \"messages\": messages}\n",
    "    )\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Increment\n",
    "    iterations = iterations + 1\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "def code_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    Check code\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    # Get solution components\n",
    "    imports = code_solution.imports\n",
    "    code = code_solution.code\n",
    "\n",
    "    # Check imports\n",
    "    try:\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        exec(imports + \"\\n\" + code)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # No errors\n",
    "    print(\"---NO CODE TEST FAILURES---\")\n",
    "    return {\n",
    "        \"generation\": code_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": \"no\",\n",
    "    }\n",
    "\n",
    "\n",
    "def reflect(state: GraphState):\n",
    "    \"\"\"\n",
    "    Reflect on errors\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "\n",
    "    # Prompt reflection\n",
    "\n",
    "    # Add reflection\n",
    "    reflections = code_gen_chain.invoke(\n",
    "        {\"context\": result, \"messages\": messages}\n",
    "    )\n",
    "    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to finish.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    if error == \"no\" or iterations == max_iterations:\n",
    "        print(\"---DECISION: FINISH---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        if flag == \"reflect\":\n",
    "            return \"reflect\"\n",
    "        else:\n",
    "            return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"generate\", generate)  # generation solution\n",
    "workflow.add_node(\"check_code\", code_check)  # check code\n",
    "workflow.add_node(\"reflect\", reflect)  # reflect\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"generate\")\n",
    "workflow.add_edge(\"generate\", \"check_code\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_code\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"reflect\": \"reflect\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"reflect\", \"generate\")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: FINISH---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': 'yes',\n",
       " 'messages': [('user',\n",
       "   'How do I build a parallel branch in LangGraph? and how do I use it to build a RAG chain?'),\n",
       "  ('assistant',\n",
       "   'To build a parallel branch in LangGraph, you can use the fan-out and fan-in mechanism. This allows you to execute multiple nodes in parallel and then combine their results. Here\\'s a basic example of how to set up a parallel branch in LangGraph. For building a RAG (Retrieval-Augmented Generation) chain, you would typically have nodes that handle retrieval and generation separately, and you can use parallel branches to handle multiple retrievals or generations simultaneously. Below is a simple example of setting up a parallel branch in LangGraph. \\n Imports: import operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END \\n Code: class State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\nclass ReturnNodeValue:\\n    def __init__(self, node_secret: str):\\n        self._value = node_secret\\n\\n    def __call__(self, state: State) -> Any:\\n        print(f\"Adding {self._value} to {state[\\'aggregate\\']}\")\\n        return {\"aggregate\": [self._value]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"a\", ReturnNodeValue(\"I\\'m A\"))\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_node(\"b\", ReturnNodeValue(\"I\\'m B\"))\\nbuilder.add_node(\"c\", ReturnNodeValue(\"I\\'m C\"))\\nbuilder.add_node(\"d\", ReturnNodeValue(\"I\\'m D\"))\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\n# To visualize the graph\\nfrom IPython.display import Image, display\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n# Invoke the graph\\nresult = graph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\\nprint(result)'),\n",
       "  ('user',\n",
       "   \"Your solution failed the code execution test: name 'State' is not defined\"),\n",
       "  ('user',\n",
       "   'Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:'),\n",
       "  ('assistant',\n",
       "   'To build a parallel branch in LangGraph, you can use the fan-out and fan-in mechanism. This allows you to execute multiple nodes in parallel and then combine their results. Here\\'s a basic example of how to set up a parallel branch in LangGraph. For building a RAG (Retrieval-Augmented Generation) chain, you would typically have nodes that handle retrieval and generation separately, and you can use parallel branches to handle multiple retrievals or generations simultaneously. Below is a simple example of setting up a parallel branch in LangGraph. \\n Imports: import operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END \\n Code: class State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\nclass ReturnNodeValue:\\n    def __init__(self, node_secret: str):\\n        self._value = node_secret\\n\\n    def __call__(self, state: State) -> Any:\\n        print(f\"Adding {self._value} to {state[\\'aggregate\\']}\")\\n        return {\"aggregate\": [self._value]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"a\", ReturnNodeValue(\"I\\'m A\"))\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_node(\"b\", ReturnNodeValue(\"I\\'m B\"))\\nbuilder.add_node(\"c\", ReturnNodeValue(\"I\\'m C\"))\\nbuilder.add_node(\"d\", ReturnNodeValue(\"I\\'m D\"))\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\n# To visualize the graph\\nfrom IPython.display import Image, display\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n# Invoke the graph\\nresult = graph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\\nprint(result)'),\n",
       "  ('user',\n",
       "   \"Your solution failed the code execution test: name 'State' is not defined\"),\n",
       "  ('user',\n",
       "   'Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:'),\n",
       "  ('assistant',\n",
       "   'To build a parallel branch in LangGraph, you can use the fan-out and fan-in mechanism. This allows you to execute multiple nodes in parallel and then combine their results. Here\\'s a basic example of how to set up a parallel branch in LangGraph. For building a RAG (Retrieval-Augmented Generation) chain, you would typically have nodes that handle retrieval and generation separately, and you can use parallel branches to handle multiple retrievals or generations simultaneously. Below is a simple example of setting up a parallel branch in LangGraph. \\n Imports: import operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display \\n Code: class State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\nclass ReturnNodeValue:\\n    def __init__(self, node_secret: str):\\n        self._value = node_secret\\n\\n    def __call__(self, state: State) -> Any:\\n        print(f\"Adding {self._value} to {state[\\'aggregate\\']}\")\\n        return {\"aggregate\": [self._value]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"a\", ReturnNodeValue(\"I\\'m A\"))\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_node(\"b\", ReturnNodeValue(\"I\\'m B\"))\\nbuilder.add_node(\"c\", ReturnNodeValue(\"I\\'m C\"))\\nbuilder.add_node(\"d\", ReturnNodeValue(\"I\\'m D\"))\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nresult = graph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\\nprint(result)'),\n",
       "  ('user',\n",
       "   \"Your solution failed the code execution test: name 'State' is not defined\")],\n",
       " 'generation': code(prefix=\"To build a parallel branch in LangGraph, you can use the fan-out and fan-in mechanism. This allows you to execute multiple nodes in parallel and then combine their results. Here's a basic example of how to set up a parallel branch in LangGraph. For building a RAG (Retrieval-Augmented Generation) chain, you would typically have nodes that handle retrieval and generation separately, and you can use parallel branches to handle multiple retrievals or generations simultaneously. Below is a simple example of setting up a parallel branch in LangGraph.\", imports='import operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display', code='class State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\nclass ReturnNodeValue:\\n    def __init__(self, node_secret: str):\\n        self._value = node_secret\\n\\n    def __call__(self, state: State) -> Any:\\n        print(f\"Adding {self._value} to {state[\\'aggregate\\']}\")\\n        return {\"aggregate\": [self._value]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"a\", ReturnNodeValue(\"I\\'m A\"))\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_node(\"b\", ReturnNodeValue(\"I\\'m B\"))\\nbuilder.add_node(\"c\", ReturnNodeValue(\"I\\'m C\"))\\nbuilder.add_node(\"d\", ReturnNodeValue(\"I\\'m D\"))\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\nresult = graph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\\nprint(result)'),\n",
       " 'iterations': 3}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How do I build a parallel branch in LangGraph? and how do I use it to build a RAG chain?\"\n",
    "solution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running generated code...\n",
      "Execution Output: expected output\n",
      "\n",
      "Running tests...\n",
      "Test Output:\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.3, pluggy-1.5.0 -- /home/kenji/mambaforge/envs/codegen/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/kenji/hack-projects/IdeaGenerator\n",
      "plugins: anyio-4.6.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "generated/test_generated_code.py::test_example \u001b[31mFAILED\u001b[0m\u001b[31m                    [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_example _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_example\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Basic example test to check if code runs without error\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m generated_code2.function() == \u001b[33m\"\u001b[39;49;00m\u001b[33mexpected output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'expected output' == 'expected outp u t'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         - expected outp u t\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?              - -\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + expected output\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mgenerated/test_generated_code.py\u001b[0m:7: AssertionError\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../mambaforge/envs/codegen/lib/python3.11/site-packages/_pytest/config/__init__.py:1277\n",
      "  /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages/_pytest/config/__init__.py:1277: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: anyio\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m generated/test_generated_code.py::\u001b[1mtest_example\u001b[0m - AssertionError: assert 'expected output' == 'expected outp u t'\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m1 warning\u001b[0m\u001b[31m in 0.00s\u001b[0m\u001b[31m =========================\u001b[0m\n",
      "\n",
      "Test result code: 1\n",
      "Test result: 1, \u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.3, pluggy-1.5.0 -- /home/kenji/mambaforge/envs/codegen/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/kenji/hack-projects/IdeaGenerator\n",
      "plugins: anyio-4.6.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "generated/test_generated_code.py::test_example \u001b[31mFAILED\u001b[0m\u001b[31m                    [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_example _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_example\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Basic example test to check if code runs without error\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m generated_code2.function() == \u001b[33m\"\u001b[39;49;00m\u001b[33mexpected output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'expected output' == 'expected outp u t'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         - expected outp u t\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?              - -\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + expected output\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mgenerated/test_generated_code.py\u001b[0m:7: AssertionError\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../mambaforge/envs/codegen/lib/python3.11/site-packages/_pytest/config/__init__.py:1277\n",
      "  /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages/_pytest/config/__init__.py:1277: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: anyio\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m generated/test_generated_code.py::\u001b[1mtest_example\u001b[0m - AssertionError: assert 'expected output' == 'expected outp u t'\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m1 warning\u001b[0m\u001b[31m in 0.00s\u001b[0m\u001b[31m =========================\u001b[0m\n",
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.3, pluggy-1.5.0 -- /home/kenji/mambaforge/envs/codegen/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/kenji/hack-projects/IdeaGenerator\n",
      "plugins: anyio-4.6.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "generated/test_generated_code.py::test_example \u001b[31mFAILED\u001b[0m\u001b[31m                    [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_example _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_example\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Basic example test to check if code runs without error\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m generated_code2.function() == \u001b[33m\"\u001b[39;49;00m\u001b[33mexpected output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'expected output' == 'expected outp u t'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         - expected outp u t\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?              - -\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + expected output\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mgenerated/test_generated_code.py\u001b[0m:7: AssertionError\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../mambaforge/envs/codegen/lib/python3.11/site-packages/_pytest/config/__init__.py:1277\n",
      "  /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages/_pytest/config/__init__.py:1277: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: anyio\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m generated/test_generated_code.py::\u001b[1mtest_example\u001b[0m - AssertionError: assert 'expected output' == 'expected outp u t'\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m1 warning\u001b[0m\u001b[31m in 0.00s\u001b[0m\u001b[31m =========================\u001b[0m\n",
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.3, pluggy-1.5.0 -- /home/kenji/mambaforge/envs/codegen/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/kenji/hack-projects/IdeaGenerator\n",
      "plugins: anyio-4.6.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "generated/test_generated_code.py::test_example \u001b[31mFAILED\u001b[0m\u001b[31m                    [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_example _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_example\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Basic example test to check if code runs without error\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m generated_code2.function() == \u001b[33m\"\u001b[39;49;00m\u001b[33mexpected output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'expected output' == 'expected outp u t'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         - expected outp u t\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?              - -\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + expected output\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mgenerated/test_generated_code.py\u001b[0m:7: AssertionError\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../mambaforge/envs/codegen/lib/python3.11/site-packages/_pytest/config/__init__.py:1277\n",
      "  /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages/_pytest/config/__init__.py:1277: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: anyio\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m generated/test_generated_code.py::\u001b[1mtest_example\u001b[0m - AssertionError: assert 'expected output' == 'expected outp u t'\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m1 warning\u001b[0m\u001b[31m in 0.00s\u001b[0m\u001b[31m =========================\u001b[0m\n",
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.10, pytest-8.3.3, pluggy-1.5.0 -- /home/kenji/mambaforge/envs/codegen/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/kenji/hack-projects/IdeaGenerator\n",
      "plugins: anyio-4.6.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "generated/test_generated_code.py::test_example \u001b[31mFAILED\u001b[0m\u001b[31m                    [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_example _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_example\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Basic example test to check if code runs without error\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m generated_code2.function() == \u001b[33m\"\u001b[39;49;00m\u001b[33mexpected output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'expected output' == 'expected outp u t'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         \u001b[0m\n",
      "\u001b[1m\u001b[31mE         - expected outp u t\u001b[0m\n",
      "\u001b[1m\u001b[31mE         ?              - -\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + expected output\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mgenerated/test_generated_code.py\u001b[0m:7: AssertionError\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../mambaforge/envs/codegen/lib/python3.11/site-packages/_pytest/config/__init__.py:1277\n",
      "  /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages/_pytest/config/__init__.py:1277: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: anyio\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m generated/test_generated_code.py::\u001b[1mtest_example\u001b[0m - AssertionError: assert 'expected output' == 'expected outp u t'\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m1 warning\u001b[0m\u001b[31m in 0.00s\u001b[0m\u001b[31m =========================\u001b[0m\n",
      "\n",
      "The capital of France is **Paris**.\n",
      "The capital of France is **Paris**.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Designing an optimal city layout for the next 50 years requires a holistic approach that integrates sustainability, accommodates population growth, promotes economic activity, and prioritizes public well-being, all while considering climate change, technological advancements, and social equity. Here's a conceptual framework outlining key components and strategies:\n",
      "\n",
      "1. **Sustainable Urban Planning:**\n",
      "   - **Compact and Mixed-Use Development:** Encourage high-density, mixed-use neighborhoods to reduce urban sprawl, promote walkability, and decrease reliance on private vehicles. \n",
      "   - **Green Spaces and Biodiversity:** Integrate parks, green roofs, and urban forests to enhance urban biodiversity, provide recreational spaces, and mitigate urban heat.\n",
      "   - **Water Management:** Design water-efficient landscapes and infrastructure to manage stormwater runoff and prevent flooding.\n",
      "\n",
      "2. **Infrastructure and Mobility:**\n",
      "   - **Public Transportation:** Develop a comprehensive, accessible, and efficient public transportation network including buses, trams, and trains, powered by renewable energy.\n",
      "   - **Smart Mobility Solutions:** Incorporate technologies like autonomous vehicles, app-based ride-sharing, and e-scooters to provide flexible and efficient transport options.\n",
      "   - **Bicycle and Pedestrian Networks:** Create extensive bicycle lanes and pedestrian-friendly pathways to encourage non-motorized transport.\n",
      "\n",
      "3. **Climate Resilience:**\n",
      "   - **Resilient Building Standards:** Implement robust building codes that promote energy efficiency and resilience to weather extremes, such as floods and heatwaves.\n",
      "   - **Renewable Energy Integration:** Utilize solar panels, wind turbines, and other renewable technologies in public and private spaces to reduce carbon footprint.\n",
      "   - **Adaptation Strategies:** Plan for sea-level rise, changing weather patterns, and resource scarcity by designing adaptive infrastructure and community response plans.\n",
      "\n",
      "4. **Technological Integration:**\n",
      "   - **Smart City Technologies:** Use data analytics, IoT, and AI to manage resources efficiently, from traffic flow to energy consumption and healthcare services.\n",
      "   - **Digital Connectivity:** Ensure widespread and equitable access to high-speed internet to facilitate remote work, online education, and telehealth services.\n",
      "\n",
      "5. **Economic and Social Considerations:**\n",
      "   - **Diverse Economic Opportunities:** Foster a diversified economy through education, entrepreneurship support, and incentives for green industries and technologies.\n",
      "   - **Affordable Housing:** Ensure a range of housing options that are accessible to all income levels, integrating affordable housing into all neighborhoods.\n",
      "   - **Community Engagement and Equity:** Establish participatory planning processes to involve diverse community voices, ensuring developments reflect the needs of all social groups.\n",
      "\n",
      "6. **Public\n",
      "As of my knowledge cutoff in October 2023, **France has not left the European Union (EU)** and remains a member state. France is one of the founding members of the EU and continues to play a significant role in its political, economic, and social structures.\n",
      "\n",
      "If you've come across information suggesting that France has left the EU, it might be based on a misunderstanding or misinformation. It's also possible that political movements or parties within France have advocated for leaving the EU at various times, similar to the UK's Brexit movement, but such initiatives have not resulted in France departing from the union.\n",
      "\n",
      "For the most accurate and up-to-date information, I recommend checking reliable news sources or official government statements.\n",
      "\n",
      "I'm sorry, but I don't have specific information about LangGraph. It's possible that it's a tool or platform that emerged after my last update in October 2023, or it might be a specialized tool not widely documented. If you can provide more details or context about LangGraph, I'd be happy to try to help further!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pytest\n",
    "\n",
    "def write_generated_code(code):\n",
    "    # Write the generated code into the placeholder file\n",
    "    with open(\"generated/generated_code.py\", \"w\") as file:\n",
    "        file.write(code)\n",
    "\n",
    "def run_generated_code():\n",
    "    # Execute the generated code in a separate process\n",
    "    result = subprocess.run([\"python\", \"generated/generated_code.py\"], capture_output=True, text=True)\n",
    "    print(\"Execution Output:\", result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"Execution Error:\", result.stderr)\n",
    "    return result.stdout\n",
    "\n",
    "def run_tests():\n",
    "    # Run tests programmatically with pytest and capture output\n",
    "    import io\n",
    "    import sys\n",
    "\n",
    "    # Redirect stdout and stderr to a buffer\n",
    "    captured_output = io.StringIO()\n",
    "    sys.stdout = captured_output\n",
    "    sys.stderr = captured_output\n",
    "\n",
    "    # Run pytest\n",
    "    result = pytest.main([\"-v\", \"generated/test_generated_code.py\"])\n",
    "\n",
    "    # Restore stdout and stderr\n",
    "    sys.stdout = sys.__stdout__\n",
    "    sys.stderr = sys.__stderr__\n",
    "\n",
    "    # Get the captured output (including stack traces)\n",
    "    output = captured_output.getvalue()\n",
    "\n",
    "    print(\"Test Output:\")\n",
    "    print(output)\n",
    "    print(f\"Test result code: {result}\")\n",
    "\n",
    "    return result, output\n",
    "\n",
    "# Generated Python code to be written to generated_code.py\n",
    "generated_code = \"\"\"\n",
    "def function():\n",
    "    return \"expected output\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(function())\n",
    "\"\"\"\n",
    "\n",
    "# Write the generated code to file\n",
    "write_generated_code(generated_code)\n",
    "\n",
    "# Run the generated code\n",
    "print(\"Running generated code...\")\n",
    "output = run_generated_code()\n",
    "\n",
    "# Run the tests\n",
    "print(\"Running tests...\")\n",
    "result, test_output = run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test result: {result}, {test_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExitCode.TESTS_FAILED: 1>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x1b[1m============================= test session starts ==============================\\x1b[0m\\nplatform linux -- Python 3.11.10, pytest-8.3.3, pluggy-1.5.0 -- /home/kenji/mambaforge/envs/codegen/bin/python\\ncachedir: .pytest_cache\\nrootdir: /home/kenji/hack-projects/IdeaGenerator\\nplugins: anyio-4.6.0\\n\\x1b[1mcollecting ... \\x1b[0mcollected 1 item\\n\\ngenerated/test_generated_code.py::test_example \\x1b[31mFAILED\\x1b[0m\\x1b[31m                    [100%]\\x1b[0m\\n\\n=================================== FAILURES ===================================\\n\\x1b[31m\\x1b[1m_________________________________ test_example _________________________________\\x1b[0m\\n\\n    \\x1b[0m\\x1b[94mdef\\x1b[39;49;00m \\x1b[92mtest_example\\x1b[39;49;00m():\\x1b[90m\\x1b[39;49;00m\\n        \\x1b[90m# Basic example test to check if code runs without error\\x1b[39;49;00m\\x1b[90m\\x1b[39;49;00m\\n>       \\x1b[94massert\\x1b[39;49;00m generated_code2.function() == \\x1b[33m\"\\x1b[39;49;00m\\x1b[33mexpected output\\x1b[39;49;00m\\x1b[33m\"\\x1b[39;49;00m\\x1b[90m\\x1b[39;49;00m\\n\\x1b[1m\\x1b[31mE       AssertionError: assert \\'expected output\\' == \\'expected outp u t\\'\\x1b[0m\\n\\x1b[1m\\x1b[31mE         \\x1b[0m\\n\\x1b[1m\\x1b[31mE         - expected outp u t\\x1b[0m\\n\\x1b[1m\\x1b[31mE         ?              - -\\x1b[0m\\n\\x1b[1m\\x1b[31mE         + expected output\\x1b[0m\\n\\n\\x1b[1m\\x1b[31mgenerated/test_generated_code.py\\x1b[0m:7: AssertionError\\n\\x1b[33m=============================== warnings summary ===============================\\x1b[0m\\n../../mambaforge/envs/codegen/lib/python3.11/site-packages/_pytest/config/__init__.py:1277\\n  /home/kenji/mambaforge/envs/codegen/lib/python3.11/site-packages/_pytest/config/__init__.py:1277: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: anyio\\n    self._mark_plugins_for_rewrite(hook)\\n\\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\\n\\x1b[36m\\x1b[1m=========================== short test summary info ============================\\x1b[0m\\n\\x1b[31mFAILED\\x1b[0m generated/test_generated_code.py::\\x1b[1mtest_example\\x1b[0m - AssertionError: assert \\'expected output\\' == \\'expected outp u t\\'\\n\\x1b[31m========================= \\x1b[31m\\x1b[1m1 failed\\x1b[0m, \\x1b[33m1 warning\\x1b[0m\\x1b[31m in 0.00s\\x1b[0m\\x1b[31m =========================\\x1b[0m\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthetic-code-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
